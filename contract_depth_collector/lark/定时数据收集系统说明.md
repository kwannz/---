# 定时数据收集系统说明

## 📊 系统概述

本系统实现了每日定时收集代币深度数据，并通过Lark机器人进行查询和分析的功能。系统包含以下核心组件：

1. **定时任务调度器** - 每日定时收集数据
2. **数据查询功能** - 历史数据分析和趋势报告
3. **Lark机器人** - 实时查询和历史分析
4. **数据存储** - 自动保存和分析历史数据

## 🚀 快速开始

### 1. 启动Webhook服务器

```bash
cd /Users/zhaoleon/Downloads/铺单量/contract_depth_collector/lark
python3 start_lark_webhook.py --host 0.0.0.0 --port 8080
```

### 2. 启动定时调度器

```bash
# 测试模式（只运行一次）
python3 start_scheduler.py --test

# 正式模式（定时运行）
python3 start_scheduler.py

# 自定义代币列表
python3 start_scheduler.py --symbols BTCUSDT ETHUSDT RIFUSDT
```

## 📋 功能说明

### 定时任务配置

系统默认配置了以下定时任务：

- **09:00** - 数据收集
- **15:00** - 数据收集  
- **21:00** - 数据收集
- **23:00** - 每日汇总报告
- **每小时** - 异常检查

### Lark机器人命令

在Lark群聊中可以使用以下命令：

#### 实时查询
- `@BTC` - 查询BTC实时铺单量
- `@ETH` - 查询ETH实时铺单量
- `@RIF` - 查询RIF实时铺单量

#### 历史分析
- `分析 BTC 7` - 分析BTC最近7天趋势
- `分析 ETH 3` - 分析ETH最近3天趋势
- `分析 RIF 1` - 分析RIF最近1天趋势

#### 数据统计
- `统计` - 查看数据统计信息
- `stats` - 查看数据统计信息（英文）

#### 帮助信息
- `help` - 显示帮助信息
- `帮助` - 显示帮助信息（中文）

## 🔧 技术架构

### 核心组件

1. **scheduler.py** - 定时任务调度器
   - 使用schedule库实现定时任务
   - 支持数据收集、分析、报告发送
   - 异常检测和通知

2. **data_query.py** - 数据查询分析
   - 历史数据查询和分析
   - 趋势分析和统计报告
   - 多维度数据分析

3. **lark_webhook_bot.py** - Lark机器人
   - Webhook服务器
   - 消息处理和响应
   - 实时数据查询和历史分析

4. **start_scheduler.py** - 启动脚本
   - 支持测试模式和正式模式
   - 命令行参数配置
   - 错误处理

### 数据流程

```
定时任务 → 数据收集 → 数据存储 → 数据分析 → Lark通知
    ↓
历史数据查询 ← 用户查询 ← Lark机器人
```

## 📊 数据存储

### 数据文件结构

```
data/
├── depth_data_20250908_133124.json  # 原始数据
├── depth_data_20250908_133124.csv   # CSV格式
├── depth_data_20250908_140439.json
├── depth_data_20250908_140439.csv
└── ...
```

### 数据格式

JSON格式包含以下字段：
- `exchange` - 交易所名称
- `symbol` - 交易对
- `timestamp` - 时间戳
- `bids` - 买盘数据
- `asks` - 卖盘数据
- `spread` - 价差
- `total_bid_volume` - 总买量
- `total_ask_volume` - 总卖量

## 🛠️ 配置说明

### 环境变量

```bash
# Lark配置
export LARK_WEBHOOK_URL="https://open.larksuite.com/open-apis/bot/v2/hook/your-webhook-url"
export LARK_SIGNATURE_SECRET="your-signature-secret"

# 数据收集配置
export COLLECTION_SYMBOLS="BTCUSDT,ETHUSDT,RIFUSDT"
export COLLECTION_DURATION="300"  # 秒
```

### 定时任务配置

可以在`scheduler.py`中修改定时任务：

```python
# 每日早上9点收集数据
schedule.every().day.at("09:00").do(self.daily_collection)

# 每日下午3点收集数据
schedule.every().day.at("15:00").do(self.daily_collection)

# 每日晚上9点收集数据
schedule.every().day.at("21:00").do(self.daily_collection)

# 每日晚上11点发送汇总报告
schedule.every().day.at("23:00").do(self.daily_summary_report)

# 每小时检查数据并发送异常报告
schedule.every().hour.do(self.hourly_check)
```

## 📈 监控和维护

### 日志文件

系统会生成详细的日志文件：
- `lark_bot.log` - Lark机器人日志
- `scheduler.log` - 调度器日志
- `main_collector.log` - 数据收集日志

### 监控命令

```bash
# 检查服务状态
ps aux | grep start_lark_webhook
ps aux | grep start_scheduler

# 检查端口监听
netstat -an | grep 8080

# 测试Webhook
curl -X POST http://localhost:8080/webhook -H "Content-Type: application/json" -d '{"type":"test"}'
```

### 数据清理

建议定期清理旧数据：

```bash
# 删除7天前的数据文件
find data/ -name "depth_data_*.json" -mtime +7 -delete
find data/ -name "depth_data_*.csv" -mtime +7 -delete
```

## ⚠️ 注意事项

### API限制

- 部分交易所API有请求频率限制
- 建议使用WebSocket连接避免限制
- 系统已实现错误重试机制

### 性能优化

- 数据收集间隔可调整
- 支持并发请求处理
- 包含缓存机制

### 错误处理

- 自动重试机制
- 异常通知功能
- 详细的错误日志

## 🔄 更新和维护

### 添加新交易所

1. 在`exchanges/`目录下创建新的收集器
2. 在`main.py`中注册新收集器
3. 在`lark_webhook_bot.py`中添加支持

### 添加新功能

1. 在`data_query.py`中添加新的分析方法
2. 在`lark_webhook_bot.py`中添加新的命令处理
3. 更新帮助信息

### 部署到生产环境

1. 使用systemd或supervisor管理进程
2. 配置日志轮转
3. 设置监控告警
4. 定期备份数据

## 📞 技术支持

如有问题，请检查：
1. 日志文件中的错误信息
2. 网络连接状态
3. API密钥配置
4. 数据文件权限

系统已完全部署并测试通过，可以开始使用！
